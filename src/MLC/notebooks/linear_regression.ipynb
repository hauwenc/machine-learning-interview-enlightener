{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "Linear regression is a statistical method used to model the relationship between a dependent variable (often denoted as $y$) and one or more independent variables (often denoted as $x$). The basic idea is to find the straight line that best fits the data points in a scatter plot.\n",
    "\n",
    "The most common form is **simple linear regression**, which models two variables:\n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "where $y$ is the dependent variable, $x$ is the independent variable, $m$ is the slope, and $b$ is the intercept. \n",
    "\n",
    "Given a set of input data $\\{(x_i, y_i)\\}$, the goal of linear regression is to find the values of $m$ and $b$ that best fit the data.\n",
    "\n",
    "The values of $m$ and $b$ are chosen to minimize the **sum of squared errors** (SSE) $\\sum_i (y_i - \\hat{y}_i)^2$.\n",
    "\n",
    "Taking partial derivatives with respect to $m$ and $b$, setting them to 0, and solving yields:\n",
    "\n",
    "$$m = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i (x_i - \\bar{x})^2},\\quad b = \\bar{y} - m\\bar{x}$$\n",
    "\n",
    "**Multiple linear regression** models the relationship between multiple independent variables and one dependent variable. The best-fit hyperplane is:\n",
    "\n",
    "$$y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n = X^\\top W$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code \n",
    "### Simple linear regression \n",
    "A basic implementation in Python using least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.slope = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X).ravel()\n",
    "        y = np.asarray(y).ravel()\n",
    "        n = len(X)\n",
    "        x_mean = np.mean(X)\n",
    "        y_mean = np.mean(y)\n",
    "        numerator = 0.0\n",
    "        denominator = 0.0\n",
    "        for i in range(n):\n",
    "            numerator += (X[i] - x_mean) * (y[i] - y_mean)\n",
    "            denominator += (X[i] - x_mean) ** 2\n",
    "        self.slope = numerator / denominator\n",
    "        self.intercept = y_mean - self.slope * x_mean\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X).ravel()\n",
    "        return [self.slope * x + self.intercept for x in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(lr.slope)\n",
    "print(lr.intercept)\n",
    "y_pred = lr.predict(X)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized (Normal Equation)\n",
    "\n",
    "$$y = XW\\quad\\text{and}\\quad W = (X^\\top X)^{-1}X^\\top y$$\n",
    "\n",
    "We will add a bias column of ones to $X$ so that the first coefficient corresponds to the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionVectorized:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: (n, d) array, without bias column.\n",
    "        y: (n,) array\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y).ravel()\n",
    "        n = X.shape[0]\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])  # prepend bias column\n",
    "        self.W = np.linalg.inv(Xb.T @ Xb) @ Xb.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        n = X.shape[0]\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])  # prepend bias column in prediction too\n",
    "        return Xb @ self.W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example input data\n",
    "X = np.array([[2, 2], [4, 5], [7, 8]])\n",
    "y = np.array([9, 17, 26])\n",
    "\n",
    "# Fit vectorized model\n",
    "lv = LinearRegressionVectorized()\n",
    "lv.fit(X, y)\n",
    "print(lv.W)  # expected ~ [3., 1., 2.]\n",
    "\n",
    "# Make predictions on new data\n",
    "X_new = np.array([[10, 11], [13, 14]])\n",
    "y_pred = lv.predict(X_new)\n",
    "print(y_pred)  # expected ~ [43., 55.]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "Here are some improvements to make a gradient-descent implementation more robust:\n",
    "\n",
    "1. **Input validation**: Ensure `X` and `y` have the same length and are not empty.\n",
    "2. **Vectorization**: Use NumPy operations instead of Python loops where possible.\n",
    "3. **Regularization**: Add L2 penalty on weights (often **excluding** the bias term).\n",
    "4. **Gradient Descent**: Avoid matrix inversion on large data; learn via iterative updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, regul=0.0):\n",
    "        self.regul = float(regul)\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, num_iter=1000, verbose=False):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y).ravel()\n",
    "        if len(X) != len(y) or len(X) == 0:\n",
    "            raise ValueError(\"X and y must have the same length and cannot be empty\")\n",
    "\n",
    "        # Add bias term to X -> [1 X]\n",
    "        n = X.shape[0]\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "        # Initialize W to zeros\n",
    "        self.W = np.zeros(Xb.shape[1])\n",
    "\n",
    "        for i in range(num_iter):\n",
    "            # Predictions\n",
    "            y_pred = Xb @ self.W\n",
    "\n",
    "            # L2 regularization excluding bias term\n",
    "            W_reg = self.W.copy()\n",
    "            W_reg[0] = 0.0\n",
    "\n",
    "            # Cost (MSE + L2), shown optionally\n",
    "            cost = np.sum((y_pred - y) ** 2) + self.regul * np.sum(W_reg ** 2)\n",
    "\n",
    "            # Gradient (d/dW of MSE + L2)\n",
    "            gradients = 2 * (Xb.T @ (y_pred - y)) + 2 * self.regul * W_reg\n",
    "\n",
    "            # Update rule\n",
    "            self.W -= lr * gradients\n",
    "\n",
    "            if verbose and (i % 1000 == 0 or i == num_iter - 1):\n",
    "                print(f\"iter={i}, cost={cost:.6f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        n = X.shape[0]\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])\n",
    "        return Xb @ self.W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "gd = LinearRegressionGD(regul=0.1)\n",
    "gd.fit(X, y, lr=0.01, num_iter=10000, verbose=True)\n",
    "print(gd.W)\n",
    "y_pred = gd.predict(X)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the data from the GD example above\n",
    "X_plot = X.ravel()\n",
    "y_plot = y\n",
    "y_pred_plot = y_pred\n",
    "\n",
    "# Sort for a clean line\n",
    "order = np.argsort(X_plot)\n",
    "X_sorted = X_plot[order]\n",
    "y_pred_sorted = y_pred_plot[order]\n",
    "\n",
    "plt.scatter(X_plot, y_plot)\n",
    "plt.plot(X_sorted, y_pred_sorted)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}